<!DOCTYPE html>
<html>

<style>
	* {
		font-family: Arial, Helvetica, sans-serif;
	}

	body {
		background-color: initial;
		margin: 0 64px 0 64px;
	}


	h1 {
		color: initial;
		font-family: Arial, Helvetica, sans-serif;
		margin: 32px 0 32px 0;
	}

	h2 {
		color: initial;
		font-family: Arial, Helvetica, sans-serif;
		margin-top: 50px;
	}

	h3 {
		color: initial;
		font-family: Arial, Helvetica, sans-serif;
	}

	p {
		color: initial;
		font-family: Arial, Helvetica, sans-serif;
		text-align: justify;
	}
	code {
		  font-family: Consolas,"courier new";
		  color: black;
		  background-color: #f1f1f1;
		  padding: 4px;
		  font-size: 105%;
	}

	.row {
		margin: 32px auto 32px auto;
		max-width: 1200px;
	}

	.column {
		float: left;
		width: 33.33%;
		padding: 5px;
		box-sizing: border-box;
		text-align: center;
		color: grey;
		text-align: center;
	}

	.column img {
		width: 100%;
	}

	.row::after {
		content: "";
		clear: both;
		display: table;
		box-sizing: border-box;
	}

	.oneimg {
		margin: 32px auto 32px auto;
		max-width: 600px;
		padding: 5px;
		box-sizing: border-box;
		text-align: center;
		color: grey;
		text-align: center;

	}

	.oneimg img {
		width: 100%;
	}

	ul {
		color: initial;
		font-family: Arial, Helvetica, sans-serif;
	}

	div.references {
		color: gray;
	}
</style>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	"HTML-CSS": { linebreaks: { automatic: true } },
			SVG: { linebreaks: { automatic: true } }
	});
	</script> <!-- MATHJAX wrapping-->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <!-- MATHJAX -->
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- MATHJAX -->
<script>
	window.MathJax = {
		tex: {
			tags: 'ams'
		}
	};
</script> <!-- MATHJAX eq labeling on-->
</head>

<body>
	<h1 style="margin-bottom: 16px;"> Reproducibility of 'H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction'</h1>
	<h2 style="margin-top: 16px; margin-bottom: 64px;"> by Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola,
		Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer </h2>

	<h2>Introduction</h2>
	<p> Recently, lots of learning methods have attempted to convert 2-D images into 3-D shapes. The developed
		approaches involve two categories: model-based and model-free. 
		The paper with title 'H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction' aims to reconstruct 3-D head
		surfaces denoted by <i>S</i>, combining the advantages of model-based and model-free approaches and existing work
		in <a href="#Jeong">[1]</a> and Implicit Differentiable Renderer (IDR) <ahref="#Lior">[2] </a>. 
		This H3D-Net approach aims at estimating the full head shape using given just 3 2-D images. In order to achieve this reduced amount of input images, 
		it is needed to include prior knowledge based on DeepSDF<a href="#Jeong">[1]</a>. DeepSDF is trained using thousands of raw incomplete scans
		to learn the Signed Distance Functions (SDF), which represent the head geometry <a href="#Jeong">[1]</a>.In the following sections, the employed H3D-Net
		method will be further analysed and then, the reproducibility results are going to be presented.
	</p>

	<h2>Method</h2>
	<p>As already mentioned H3D-Net is based on training a prior, namely the already existing DeepSDF <a
			href="#Jeong">[1]</a> network and using it to initialize the already
		existing IDR <a href="#Lior">[2] </a> to reduce the needed amount of
		inputs, leading to
		the need of only 3 2-D images to get the whole 3-D head representation. Thus, H3D-Net is comprised of two steps:
	</p>

	<h3> 1. DeepSDF </h3>
	<p>The implementation of the DeepSDF network as a prior, being an already established method of representing classes
		of objects as a learned Signed Distance Function (SDF), is an effective tool to overcome intrinsic difficulties
		that pertain 3D object reconstruction from limited 2D images and therefore increases the overall efficiency. How
		does the network function, however, and what inputs does it require in order to come into effect?</p>
	<p>The key method of the DeepSDF lies on the concept of the signed distance function which can be described as a
		continuous volumetric field associated with not a single object, but an entire class of objects. The actual
		surface of the object is then implicitly given from the decision boundary of the function as seen in the figure.
	</p>
	<div class="oneimg">
		<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/ExampleSDF.png"
			alt="Snow" />
		Figure.1 Example of an SDF representation of a volume the decision boundary of which constitutes the
		object's surface.
	</div>
	<p>DeepSDF utilizes a set of scenes with the associated raw 3D point clouds and, in combination with their SDF
		values, is used to train a deep neural network that produces the SDF value at the 3D query location.</p>
	<p>Key parameters of the network:</p>
	<ul>
		<li>Eight 512-dimensional layers with ReLU non-linearities</li>
		<li>Use of latent vector as a form of encoding the desired shape as an input to the network</li>
		<li>Use of auto-decoder without an encoder.</li>
	</ul>
	<div class="oneimg">
		<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/StructureDeepSDF.png"
			alt="Snow" />
		Figure.1 Input (left) and Output (right) of the DeepSDF network.
	</div>

	<h3> 2. IDR </h3>
	<p> The goal of the Implicit Differentiable Renderer(IDR) is to reproduce the 3D geometry of an object given 2D
		views - images with noisy camera information.
	</p>
	<p>For this purpose, the IDR learns:<\p>
	<p><ol>
		<li> for each 3D point its Signed Distance Function to the object shape, which is the geometry represenation.
			Actually the geometry is represented as a zero level-set of a neural network f:
			<p class="mjax">
				\begin{equation}
				S_{\theta}=\{ x \in R^3|f(x;\theta)=0\}
				\label{eq:VsumA2}
				\end{equation}
			</p>
		</li>
		<li>the camera parameters</li>
		<li>a wide set of lighting conditions and materials, usingn a neural renderer, which actually represents the
			realistic appearances of the objects</li>
	</ol><\p>
	<p>
		The <b>training data</b> is actually real world 2D images of objects of multiple views (approximately 60 views).
		These objects are not head representations but various, random objects such as 3D skull objects, 3D rabbit
		representations and so on so forth. An idea on the used datasets can be seen in <a href="#Jeong">[1]</a>.
	</p>
	<p>
		Thus, the contribution of the neural-network architecture of IDR-system is that it can infer from masked 2D
		images,
		<b> 3D geometries, appearances </b> and <b>camera parameters </b>, under which those images were captured.
		More specifically, by appearances it is meant all the factors that define the surface light field, excluding the
		geometry, for instance, the scene's lighting conditions.
	</p>
	<p>
		Furthermore, let's give some definitions and their notations. The camera orientation, c and a pixel, p define a
		viewing direction, v. Then, given the parameters c,p we would like to produce differentiable RGB values. Additionally, we can trace the first
		intersection point, x, of the viewing ray with the implicit surface $S_{\theta}$. Finally, the direction of the ray is denoted by t and the normal to the surface vector as n. 
		All of these definitions are visually inspected on Figure.3. 
		<div class="oneimg">
		<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/IDR/IDRmethod.PNG"
			alt="Snow" />
		Figure.3 Notation Definitions
		</div>
		Nevertheless, we can calculate the intersection point, x as:
		<p class="mjax">
		         \begin{equation}  
			x = c + tv - \frac{v}{\nabla{f_0}\cdotv_0f(c+tv)}
		         \end{equation}
		<\p>
	</p>

	<h2> Reproduction </h2>

	<h2>1. Reproduction of DeepSDF prior </h2>

	<h3> Training Dataset</h3>
	<p> The original dataset used in the paper wasn't available for public use - due to confientiality issues, since the
		authors are currently collaborating with a company which owns the original data- alternatively, we produced
		thousands of 3D head samples using the FLAME 3D morphable model of heads <a href="#FLAME">[3]</a>.
		More specifically, we produced almost 10000 3D head samples, whose data type is '.obj'. One of these 10000
		samples can be seen in the following figure.

	<p>One out of 10000 produced 3D head samples:</p>

	<div class="row">
		<div class="column">
			<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/FLAME_sample_exmp.PNG?fbclid=IwAR23U9w5wZGSjZReKUUoa2jE6NoXkeCK5eo_OP7wxz2wbNc4ALq721Wj6W8"
				alt="Snow">
			Figure.1 3D head(<i>.obj</i> data type) continuous view
		</div>
		<div class="column">
			<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/FLAME_sample_exmp_mesh.png"
				alt="Forest">
			Figure.2 3D head mesh view
		</div>
		<div class="column">
			<img src="https://raw.githubusercontent.com/IoannaChano/IoannaChano.github.io/main/images/FLAME_sample_exmp_notClosed.png"
				alt="Mountains">
			Figure.3 3D head discontinuous view
		</div>
	</div>
	<p>As we can see the training data are characterized by:</p>
	<ul>
		<li>non-continuouity of the meshes, with open spaces on the bottom of the heads</li>
		<li>the heads don't have hair</li>
		<li>the heads don't have shoulders</li>
	</ul>

	<p>These are some of the contributing factors for ... </p>


	<h3> Setting up Dependencies </h3>
	<p>It is certain that setting up the github repository dependencies has been the most time consuming part of the project
		leading to much time delay, since one cannot move on to the next task if the dependencies part is not completed.
		DeepSDF requiered four main packages to be installed to perform the training. The four packages are eigen, nanoflann, CLI11 and Pangolin. </p>
	<p>We believe that if one wants to train DeepSDF, he must set up the dependecies with the least hurdle as possible. Hence, the step by step building process of the requiered packages are displayed below. If you follow the steps in the following order, one should be able to successfully build DeepSDF inside a Deep Learning VM instance of Google Cloud Platform.</p>
	<h4>Eigen</h4>	
	<ul>
		<li><code>git clone https://gitlab.com/libeigen/eigen.git</code></li>
		<li><code>cd eigen</code></li>	
		<li><code> git checkout 3.4</code></li>
		<li><code>mkdir build && cd build </code></li>
		<li><code>cmake .. </code></li>
		<li><code>cmake --build . </code></li>
		<li><code>sudo make install </code></li>
	</ul>
	<h4>Pangolin4</h4>
	<ul>			
		<li><code>git clone --recursive https://github.com/stevenlovegrove/Pangolin.git</code></li>
		<li><code>cd pangolin && ./scripts/install_prerequisites.sh recommended</code></li>	
		<li><code> git checkout v0.6</code></li>
		<li><code>mkdir build && cd build </code></li>
		<li><code>cmake .. </code></li>
		<li><code>cmake --build . </code></li>
		<li><code>sudo make install </code></li>
	</ul>
	
	<h4>CLI11</h4>
		<ul>			
		<li><code>git clone git@github.com:CLIUtils/CLI11.git</code></li>
		<li><code>cd CLI11</code></li>	
		<li><code>mkdir build && cd build </code></li>
		<li><code>cmake .. </code></li>
		<li><code>cmake --build . </code></li>
		<li><code>sudo make install </code></li>
	</ul>
		
	<h4>nanoflann</h4>
	<ul>			
		<li><code>git clone git@github.com:jlblancoc/nanoflann.git</code></li>
		<li><code>cd nanoflann</code></li>	
		<li><code>mkdir build && cd build </code></li>
		<li><code>cmake .. </code></li>
		<li><code>cmake --build . </code></li>
		<li><code>sudo make install </code></li>
	</ul>
	
	<h4>DeepSDF</h4>
		<ul>			
		<li><code>git clone git@github.com:eduardramon/DeepSDF.git</code></li>
		<li><code>cd DeepSDF</code></li>
		<li><code>git submodule update --init</code></li>	
		<li><code>mkdir build && cd build </code></li>
		<li><code>cmake .. -DCMAKE_CXX_STANDARD=17 </code></li>
		<li><code>cmake --build . </code></li>
		<li><code>sudo make install </code></li>
	</ul>
		
	
				
				
	<h3> Data Pre-processing</h3>
	<p> The aim of pre-processing the data in the DeepSDF reproduction is the input of .obj files and the output of .npz
		data type samples.</p>

	<h2>2. Reproduction of Prior aided-IDR </h2>

	<h3> Training Dataset</h3>

	<h3> Setting up Dependencies </h3>

	<h3> Data Pre-processing</h3>







	<div class="references">
		<h2> References</h2>
		<p id="Jeong">
			[1] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.
			Deepsdf: Learning continuous signed distance functions for shape representation.In CVPR, 2019
		</p>
		<p id="Lior">
			[2] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.
			Multiview neural surface reconstruction by disentangling geometry and appearance. NeurIPS, 33, 2020.
		</p>

		<p id="FLAME">
			[3] https://github.com/Rubikplayer/flame-fitting
		</p>



	</div>

</body>

</html>
